#分布式事务和分布式锁

分布式事务： 指构成事物的资源 服务不在同一个设备 CAP理论
可用性(A) + 分区容错性(P) , 就要放弃一致性(C) AC 这种组合 分布式环境中不可能共存，只适用于单机。 AP eureka保证 CP zookeeper保证

Base理论 基本可用

#分布式事物的解决方案

二阶段提交 先确认所有的子事物ok 然后提交。一般不建议使用 一致性高 但是性能消耗大。
TCC 尝试 try 检测锁定资源，confirm 执行实际的业务操作， cancle 取消中需要自己实现程序的回滚逻辑 。
本地消息表 业务处理后发送一个mq给其他系统并把消息写入本系统的表；借助数据库记录消息的发起和接收，结合MQ 决定事物是否会滚。
最终一致性方案 先预发一条消息，当系统本身的事物执行完毕在将MQ中的消息变更为i确认消息。 RocketMq事物消息
最大努力通知方案
#分布式锁 多进程中保证同步 分布式系统中的实现不同线程对代码和资源的同步访问 
实现分布式锁的方案 a. memcahe中的add命令 b. redis中的setnx命令 c.Zookeeper分布式锁 顺序临时节点

redis实现方案 分布式锁实现的三要素; 加锁 执行命令返回1 说明设置成功获得锁，返回0说明key已经存在，抢锁失败。 setnx(key ,value)
set(key,value,30,nx) 解锁 当得到锁的线程执行完毕需要释放锁 del(key) 锁超时 保证没有显示的释放锁也会在有效期后自动释放 expire(key,30) 
释放锁时得分成两步：1.判断value是否已经过期 2.如果已过期直接忽略，如果没过期就执行del（这步还是有问题 需要用lua脚本释放锁）

多个redis如何都加锁? 多写的方式， redis1加锁成功后也写到其他redis点。

分布式环境实现无启停的热加载配置中心？ watcher机制 树形结构 zkClient连接zk 自定义要给数据监听器 然后添加到client上。

ELK日志收集分析 ElasticSearch+LogStash+Kibana +kafka消息缓存队列 
通过引入 kafka的好处是 即使 LogStash中央Server故障数据还会保存，避免丢失。
 logStash的 代理 在各个服务器收集日志数据发送到kafka，并传送到LogStash中央服务器然后过滤给ES存储供用户分析查询等。

#Zookeeper
特点: 分布式、对等性、并发性、缺乏全局时钟、故障随时发生


create [-s -e ] path data acl 如 create /zk-book 123 创建 /zk-book节点 数据是123 默认创建持久节点 -s 顺序节点 -e 临时节点 ls /set
get /zk-book 获取节点下的信息 set /zk-book 456 更新节点内容



zookeeper的节点类型 znode： 
持久化目录节点 
持久化顺序编号目录节点 
临时性目录节点 客户端与zk断开连接后，节点被删除。
临时性顺序编号目录节点
>zookeeper的主要使用场景和作用？？ 
解决分布式应用中的数据管理 一致性问题 Zookeeper 作用主要是用来维护和监控存储的数据的状态变化 简单的说 zk=文件系统+通知机制

统一命名服务
分布式配置中心（数据发布订阅）
集群管理（master选举 client server）（分布通知 协调 减少系统耦合）
状态同步服务
分布式锁 实现分布式锁 的原理 抢占节点 临时节点不能有子节点
事件监听方式： 观察者模式（watcher）、 缓存监听（本地缓存视图与远程zk视图对比过程）
 Cache事件监听种类：Path、 Node、 Tree Cache 
 zookeeper的watcher机制 是实现分布式锁的主要手段 监控 data_tree 中的节点任意变化 
 新增 删除 修改 以及孩子的变化都可以在获取数据变化时注册一个watcher 获得监听的方式： 
 getChildren exists get

>zk底层原理?
zk保证强一致性，所有请求写入一个队列中，然后单线程从队列中拿出请求处理，避免并发；对于写请求zk都 交由Leader节点处理。 
leader节点处理 两阶段提交，先询问其他节点再提交；超过一半则认为可提交; 持久化事物日志，更新内存。
 2.watch机制 客户端进行定义监听器绑定到某个节点；服务的抛出事件。
领导者选举 数据同步
集群角色 leader Follower Observer 通过一次选举过程，被选举的机器节点被称为Leader，Leader机器为客户端提供读和写服务； 
Follower和Observer是集群中的其他机器节点，唯一的区别就是：Observer不参与Leader的选举过程，也不参与写操作的过半写成功策略。

#zk选举机制？ 
启动的时候做选举 、leader节点宕机重新选举、超过一半的follower节点宕机时重新选举 
zxId事物ID大的可以当作Leader节点（谁数据新谁leader）; 
如zxid都相等则zookeeper的myId大则为leader; vote(zxid,myid) 
选举流程：节点先选自己为leader，
然后不断的和其他的节点交流发现其他节点更适合当leader则改票。 每一台zk都有一个投票箱，zk之间都持有socket连接。
交流落选的投票会发送不出去。 选举的时候不能处理客户请求。 zk启动的过程：确定自己的角色、数据同步处理、

>zk如何保证一致性? -两阶段提交、领导者选举机制、过半验证机制 zookeeper实现数据一致性的核心是ZAB协议（Zookeeper原子消息广播协议）。
该协议需要做到以下几点： 
集群在半数以下节点宕机的情况下，能正常对外提供服务 客户端的写请求全部转交给leader来处理，
leader需确保写变更能实时同步给所有follower及observer； 
Zab协议有两种模式， 崩溃恢复（选主+数据同步）和消息广播（事务操作）。
任何时候都需要保证只有一个主进程负责进行事务操作，而如果主进程崩溃了， 就需要迅速选举出一个新的主进程。 
主进程的选举机制与事务操作机制是紧密相关的。下面详细讲解这三个场景的协议规则，从细节去探索ZAB协议的数据一致性原理。

通过 选主节点 、选主后数据同步 事物操作保证一致性。

zk处理请求的过程: 
client发送事物写请求 
zk master节点 的动作 生成事物日志并持久化;向其他节点发送事物日志 接受slave节点事物日志持久化结果;处理日志更新dataTree;

Paxos算法

nginx+keepAlived 结合做高可用 
主从nginx节点 有一个对外虚拟IP keepAlived session 共享 第一次访问的session保存到redis 第二次去redis查

#如何实现一个rpc框架？

http和rpc的区别？
http是通用协议但是性能比较差
rpc采用定制的协议 性能高。 通用性差。
一般rpc用于内部调用，http用户外部交互使用。


##分布式限流
思想 
计数器 单位时间内限制访问次数 
滑动窗口 把单位时间分为多个小段 滚动的时间段 
漏桶算法 数值容量上线，超过则丢弃 如线程池 信号量隔离等 
令牌桶算法 生成token令牌，请求到达时先请求令牌，如获取不到则触发限流  恒定的速度往令牌桶中添加令牌。

解决方案: redis redis设置一个key 单位时间内设置限流次数 key设置有效期 
接受到请求后 判断key是否还有效，如果有效则+1 然后和设置的限流次数比较
对需要限流的方法添加注解 拦截器中去实现以上逻辑。


##高并发下保证幂等性的方式
数据库添加唯一索引，新增的时候 先查询判断是否存在。
token机制防止页面重复提交；处理流程 a.提交前先申请token 服务器或redis保存一份 b. 数据提交 带上token 后台校验后 并删除保留的token。
悲观锁 select * from table where id ='11' for update; id必须是唯一索引或主键不然锁表了，慎用
乐观锁的方式 
a) update table_xxx set name=#name#,version=version+1 where version=#version# 
b) update tablexxx set avaiamount=avaiamount-#subAmount# where avaiamount-#subAmount#>=0
对于分布式系统很难 构建唯一索引比较困难 只有引入第三方系统 
如 redis zk 等 分布式锁实现 如 把请求种的一些唯一标识 如 用户ID+标识 去获取分布式锁
对于一些单据类的状态处理 可以用状态机制防止并发，更新数据时判断状态。

#MQ可能发生重复消息,如何解决？
产生消息重复的原因:
1. 生产者发送消息给mq没有接收到确认应答，生产者重试导致重复
2. 消息队列MQ投递给消费端的时候发生重复，比如 消费超时 mq没有得到消费结果。 mq中间件异常重启等情况 没有及时跟新消息状态
为了解决消息重复的情况需要 在消费端做消费幂等性处理。

mq如何保证消息的顺序性 
本地消息表+定时扫描重跑；顺序性不够重要，重要的是保证业务的最终一致性。 
保证一致性的方式： 做消息消费的幂等性、防止重复处理 本地消费记录表 如果消息ID已经存在则不处理。





